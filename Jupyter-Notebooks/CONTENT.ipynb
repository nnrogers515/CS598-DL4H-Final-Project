{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Recurrent network example.  Trains a bidirectional vanilla RNN to output the\n",
    "sum of two numbers in a sequence of random numbers sampled uniformly from\n",
    "[0, 1] based on a separate marker sequence.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from config import Config\n",
    "from patient_data_reader import PatientReader\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from lasagne.layers.timefusion import MaskingLayer\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, accuracy_score, precision_recall_curve\n",
    "from lasagne.layers.theta import ThetaLayer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import average_precision_score as pr_auc\n",
    "\n",
    "# Number of units in the hidden (recurrent) layer\n",
    "N_HIDDEN = 200\n",
    "# Number of training sequences in each batch\n",
    "\n",
    "\n",
    "# All gradients above this will be clipped\n",
    "GRAD_CLIP = 100\n",
    "# How often should we check the output?\n",
    "EPOCH_SIZE = 100\n",
    "# Number of epochs to train the net\n",
    "num_epochs = 6\n",
    "\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "\n",
    "def iterate_minibatches_listinputs(inputs, batchsize, shuffle=False):\n",
    "    assert inputs is not None\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [input[excerpt] for input in inputs]\n",
    "\n",
    "def loadEmbeddingMatrix(wordvecFile):\n",
    "\tfw = open(wordvecFile, \"r\")\n",
    "\theadline = fw.readline().strip().split()\n",
    "\tvocabSize = int(headline[0])\n",
    "\tdim = int(headline[1])\n",
    "\tW = np.zeros((vocabSize, dim)).astype(theano.config.floatX)\n",
    "\tfor line in fw:\n",
    "\t\ttabs = line.strip().split()\n",
    "\n",
    "\t\tvec = np.asarray([float(x) for x in tabs[1:]], dtype=theano.config.floatX)\n",
    "\t\tind = int(tabs[0])\n",
    "\t\tW[ind-1] = vec\n",
    "\tfw.close()\n",
    "\treturn W\n",
    "\n",
    "\n",
    "def main(data_sets, W_embed):\n",
    "    # Optimization learning rate\n",
    "    LEARNING_RATE = theano.shared(np.array(0.001, dtype=theano.config.floatX))\n",
    "    eta_decay = np.array(0.5, dtype=theano.config.floatX)\n",
    "    # Min/max sequence length\n",
    "    MAX_LENGTH = 300\n",
    "    X_raw_data, Y_raw_data = data_sets.get_data_from_type(\"train\")\n",
    "    trainingAdmiSeqs, trainingMask, trainingLabels, trainingLengths, ltr = prepare_data(X_raw_data, Y_raw_data, vocabsize= 619, maxlen = MAX_LENGTH)\n",
    "    Num_Samples, MAX_LENGTH, N_VOCAB = trainingAdmiSeqs.shape\n",
    "\n",
    "    X_valid_data, Y_valid_data = data_sets.get_data_from_type(\"valid\")\n",
    "    validAdmiSeqs, validMask, validLabels, validLengths, lval  = prepare_data(X_valid_data, Y_valid_data, vocabsize= 619, maxlen = MAX_LENGTH)\n",
    "\n",
    "    X_test_data, Y_test_data = data_sets.get_data_from_type(\"test\")\n",
    "    test_admiSeqs, test_mask, test_labels, testLengths, ltes = prepare_data(X_test_data, Y_test_data, vocabsize= 619, maxlen = MAX_LENGTH)\n",
    "    alllength = sum(trainingLengths) + sum(validLengths) + sum(testLengths)\n",
    "    print(alllength)\n",
    "    eventNum = sum(ltr)+sum(lval)+sum(ltes)\n",
    "    print(eventNum)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Building network ...\")\n",
    "    N_BATCH = 1\n",
    "    # First, we build the network, starting with an input layer\n",
    "    # Recurrent layers expect input of shape\n",
    "    # (batch size, max sequence length, number of features)\n",
    "    l_in = lasagne.layers.InputLayer(shape=(N_BATCH, MAX_LENGTH, N_VOCAB))\n",
    "    #l_label = lasagne.layers.InputLayer(shape=(N_BATCH, MAX_LENGTH, 1))\n",
    "\n",
    "    # The network also needs a way to provide a mask for each sequence.  We'll\n",
    "    # use a separate input layer for that.  Since the mask only determines\n",
    "    # which indices are part of the sequence for each batch entry, they are\n",
    "    # supplied as matrices of dimensionality (N_BATCH, MAX_LENGTH)\n",
    "    l_mask = lasagne.layers.InputLayer(shape=(N_BATCH, MAX_LENGTH))\n",
    "    embedsize = 100\n",
    "    n_topics = 50\n",
    "    #l_embed = lasagne.layers.DenseLayer(l_in, num_units=embedsize, b=None, W = W_embed, num_leading_axes=2)\n",
    "    l_embed = lasagne.layers.DenseLayer(l_in, num_units=embedsize, b=None, num_leading_axes=2)\n",
    "    #l_embed.params[l_embed.W].remove(\"trainable\")\n",
    "    #l_drop = lasagne.layers.dropout(l_embed)\n",
    "    l_forward0 = lasagne.layers.GRULayer(\n",
    "        l_embed, N_HIDDEN, mask_input=l_mask, grad_clipping=GRAD_CLIP,\n",
    "        only_return_final=False)\n",
    "\n",
    "    l_forward = MaskingLayer([l_forward0, l_mask])\n",
    "\n",
    "    l_1 = lasagne.layers.DenseLayer(l_in, num_units=N_HIDDEN, nonlinearity=lasagne.nonlinearities.rectify, num_leading_axes=2)\n",
    "    l_2 = lasagne.layers.DenseLayer(l_1, num_units=N_HIDDEN, nonlinearity=lasagne.nonlinearities.rectify, num_leading_axes=2)\n",
    "    mu = lasagne.layers.DenseLayer(l_2, num_units=n_topics, nonlinearity=None, num_leading_axes=1)# batchsize * n_topic\n",
    "    log_sigma = lasagne.layers.DenseLayer(l_2, num_units=n_topics, nonlinearity=None, num_leading_axes=1)# batchsize * n_topic\n",
    "    l_theta = ThetaLayer([mu,log_sigma],maxlen=MAX_LENGTH)#batchsize * maxlen * n_topic\n",
    "\n",
    "    l_B = lasagne.layers.DenseLayer(l_in, b=None, num_units=n_topics, nonlinearity=None, num_leading_axes=2)\n",
    "    l_context = lasagne.layers.ElemwiseMergeLayer([l_B, l_theta],T.mul)\n",
    "    l_context = lasagne.layers.ExpressionLayer(l_context, lambda X: X.mean(-1), output_shape=\"auto\")\n",
    "\n",
    "    l_dense0 = lasagne.layers.DenseLayer(\n",
    "        l_forward, num_units=1, nonlinearity=None,num_leading_axes=2)\n",
    "    l_dense1 = lasagne.layers.reshape(l_dense0, ([0], [1]))#batchsize * maxlen\n",
    "    l_dense = lasagne.layers.ElemwiseMergeLayer([l_dense1, l_context],T.add)\n",
    "    l_out0 = lasagne.layers.NonlinearityLayer(l_dense, nonlinearity=lasagne.nonlinearities.sigmoid)\n",
    "    l_out = lasagne.layers.ExpressionLayer(lasagne.layers.ElemwiseMergeLayer([l_out0, l_mask],T.mul), lambda X:X+0.000001)\n",
    "\n",
    "\n",
    "\n",
    "    target_values = T.matrix('target_output')\n",
    "    target_values_flat = T.flatten(target_values)\n",
    "\n",
    "    # lasagne.layers.get_output produces a variable for the output of the net\n",
    "    network_output = lasagne.layers.get_output(l_out)\n",
    "    # The network output will have shape (n_batch, maxlen); let's flatten to get a\n",
    "    # 1-dimensional vector of predicted values\n",
    "    predicted_values = network_output.flatten()\n",
    "    # Our cost will be mean-squared error\n",
    "    cost = lasagne.objectives.binary_crossentropy(predicted_values, target_values_flat)\n",
    "    kl_term = l_theta.klterm\n",
    "    cost = cost.sum()+kl_term\n",
    "\n",
    "\n",
    "    test_output = lasagne.layers.get_output(l_out, deterministic=True)\n",
    "\n",
    "    #cost = T.mean((predicted_values - target_values)**2)\n",
    "    # Retrieve all parameters from the network\n",
    "    all_params = lasagne.layers.get_all_params(l_out)\n",
    "\n",
    "    # Compute SGD updates for training\n",
    "    print(\"Computing updates ...\")\n",
    "    updates = lasagne.updates.adam(cost, all_params, LEARNING_RATE)\n",
    "    # Theano functions for training and computing cost\n",
    "    print(\"Compiling functions ...\")\n",
    "    train = theano.function([l_in.input_var, target_values, l_mask.input_var],\n",
    "                            cost, updates=updates)\n",
    "    compute_cost = theano.function(\n",
    "        [l_in.input_var, target_values, l_mask.input_var],cost)\n",
    "    prd = theano.function([l_in.input_var, l_mask.input_var], test_output)\n",
    "    #rnn_out = T.concatenate(l_theta.theta, lasagne.layers.get_output(l_forward0)[:,-1,:].reshape((N_BATCH, N_HIDDEN)),axis=1)\n",
    "    output_theta = theano.function([l_in.input_var, l_mask.input_var], [l_theta.theta, lasagne.layers.get_output(l_forward0)[:,-1,:].reshape((N_BATCH, N_HIDDEN))], on_unused_input='ignore')\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Training ...\")\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            train_err = 0\n",
    "            train_batches = 0\n",
    "            start_time = time.time()\n",
    "            thetas_train = []\n",
    "            for batch in iterate_minibatches_listinputs([trainingAdmiSeqs, trainingLabels, trainingMask], N_BATCH,\n",
    "                                                        shuffle=True):\n",
    "                inputs = batch\n",
    "                train_err += train(inputs[0], inputs[1], inputs[2])\n",
    "                train_batches += 1\n",
    "                theta_train, rnnvec_train = output_theta(inputs[0], inputs[2])\n",
    "                rnnout_train = np.concatenate([theta_train, rnnvec_train], axis=1)\n",
    "                thetas_train.append(rnnout_train.flatten())\n",
    "                if (train_batches+1)% 1000 == 0:\n",
    "                    print(train_batches)\n",
    "\n",
    "\n",
    "            np.save(\"theta_with_rnnvec/thetas_train\"+str(epoch),thetas_train)\n",
    "\n",
    "\n",
    "            # # And a full pass over the validation data:\n",
    "            # val_err = 0\n",
    "            # val_acc = 0\n",
    "            # val_batches = 0\n",
    "            # new_validlabels = []\n",
    "            # pred_validlabels = []\n",
    "            # for batch in iterate_minibatches_listinputs([validAdmiSeqs, validLabels, validMask, validLengths], 1, shuffle=False):\n",
    "            #     inputs = batch\n",
    "            #     err = compute_cost(inputs[0], inputs[1], inputs[2])\n",
    "            #     val_err += err\n",
    "            #     leng = inputs[3][0]\n",
    "            #     new_validlabels.extend(inputs[1].flatten()[:leng])\n",
    "            #     pred_validlabels.extend(prd(inputs[0], inputs[2]).flatten()[:leng])\n",
    "            #     val_batches += 1\n",
    "            # val_auc = roc_auc_score(new_validlabels, pred_validlabels)\n",
    "            # Then we print the results for this epoch:\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                epoch + 1, num_epochs, time.time() - start_time))\n",
    "            print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "            # print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "            # print(\"  validation auc:\\t\\t{:.6f}\".format(val_auc))\n",
    "            # print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            #     val_acc / val_batches * 100))\n",
    "\n",
    "            # After training, we compute and print the test error:\n",
    "            test_err = 0\n",
    "\n",
    "            test_batches = 0\n",
    "            new_testlabels = []\n",
    "            pred_testlabels = []\n",
    "            thetas = []\n",
    "            for batch in iterate_minibatches_listinputs([test_admiSeqs, test_labels, test_mask, testLengths], 1, shuffle=False):\n",
    "                inputs = batch\n",
    "                err = compute_cost(inputs[0], inputs[1], inputs[2])\n",
    "                test_err += err\n",
    "                leng = inputs[3][0]\n",
    "                new_testlabels.extend(inputs[1].flatten()[:leng])\n",
    "                pred_testlabels.extend(prd(inputs[0], inputs[2]).flatten()[:leng])\n",
    "                theta, rnnvec = output_theta(inputs[0], inputs[2])\n",
    "                rnnout = np.concatenate([theta, rnnvec],axis=1)\n",
    "                thetas.append(rnnout.flatten())\n",
    "                test_batches += 1\n",
    "            test_auc = roc_auc_score(new_testlabels, pred_testlabels)\n",
    "            test_pr_auc = pr_auc(new_testlabels, pred_testlabels)\n",
    "            # np.save(\"CONTENT_results/testlabels_\"+str(epoch),new_testlabels)\n",
    "            # np.save(\"CONTENT_results/predlabels_\"+str(epoch),pred_testlabels)\n",
    "            # np.save(\"CONTENT_results/thetas\"+str(epoch),thetas)\n",
    "\n",
    "            # np.save(\"theta_with_rnnvec/testlabels_\"+str(epoch),new_testlabels)\n",
    "            # np.save(\"theta_with_rnnvec/predlabels_\"+str(epoch),pred_testlabels)\n",
    "            # np.save(\"theta_with_rnnvec/thetas\"+str(epoch),thetas)\n",
    "\n",
    "\n",
    "            test_pre_rec_f1 = precision_recall_fscore_support(np.array(new_testlabels), np.array(pred_testlabels)>0.5, average='binary')\n",
    "            test_acc = accuracy_score(np.array(new_testlabels), np.array(pred_testlabels)>0.5)\n",
    "            print(\"Final results:\")\n",
    "            print(\"  test loss:\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "            print(\"  test auc:\\t\\t{:.6f}\".format(test_auc))\n",
    "            print(\"  test pr_auc:\\t\\t{:.6f}\".format(test_pr_auc))\n",
    "            print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "                test_acc * 100))\n",
    "            print(\"  test Precision, Recall and F1:\\t\\t{:.4f} %\\t\\t{:.4f}\\t\\t{:.4f}\".format(test_pre_rec_f1[0], test_pre_rec_f1[1], test_pre_rec_f1[2]))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "\n",
    "def prepare_data(seqs, labels, vocabsize, maxlen=None):\n",
    "    \"\"\"Create the matrices from the datasets.\n",
    "\n",
    "    This pad each sequence to the same lenght: the lenght of the\n",
    "    longuest sequence or maxlen.\n",
    "\n",
    "    if maxlen is set, we will cut all sequence to this maximum\n",
    "    lenght.\n",
    "\n",
    "    This swap the axis!\n",
    "    \"\"\"\n",
    "    # x: a list of sentences\n",
    "    lengths = [len(s) for s in seqs]\n",
    "\n",
    "    eventSeq = []\n",
    "\n",
    "    for seq in seqs:\n",
    "        t = []\n",
    "        for visit in seq:\n",
    "            t.extend(visit)\n",
    "        eventSeq.append(t)\n",
    "    eventLengths = [len(s) for s in eventSeq]\n",
    "\n",
    "\n",
    "    if maxlen is not None:\n",
    "        new_seqs = []\n",
    "        new_lengths = []\n",
    "        new_labels = []\n",
    "        for l, s, la in zip(lengths, seqs, labels):\n",
    "            if l < maxlen:\n",
    "                new_seqs.append(s)\n",
    "                new_lengths.append(l)\n",
    "                new_labels.append(la)\n",
    "            else:\n",
    "                new_seqs.append(s[:maxlen])\n",
    "                new_lengths.append(maxlen)\n",
    "                new_labels.append(la[:maxlen])\n",
    "        lengths = new_lengths\n",
    "        seqs = new_seqs\n",
    "        labels = new_labels\n",
    "\n",
    "        if len(lengths) < 1:\n",
    "            return None, None, None\n",
    "\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((n_samples, maxlen, vocabsize)).astype('int64')\n",
    "    x_mask = np.zeros((n_samples, maxlen)).astype(theano.config.floatX)\n",
    "    y = np.ones((n_samples, maxlen)).astype(theano.config.floatX)\n",
    "    for idx, s in enumerate(seqs):\n",
    "        x_mask[idx, :lengths[idx]] = 1\n",
    "        for j, sj in enumerate(s):\n",
    "            for tsj in sj:\n",
    "                x[idx, j, tsj-1] = 1\n",
    "    for idx, t in enumerate(labels):\n",
    "        y[idx,:lengths[idx]] = t\n",
    "        # if lengths[idx] < maxlen:\n",
    "        #     y[idx,lengths[idx]:] = t[-1]\n",
    "\n",
    "    return x, x_mask, y, lengths, eventLengths\n",
    "\n",
    "\n",
    "\n",
    "def eval(epoch):\n",
    "    new_testlabels = np.load(\"CONTENT_results/testlabels_\"+str(epoch)+\"_3999.npy\")\n",
    "    pred_testlabels = np.load(\"CONTENT_results/predlabels_\"+str(epoch)+\"_3999.npy\")\n",
    "    test_auc = roc_auc_score(new_testlabels, pred_testlabels)\n",
    "    test_pr_auc = pr_auc(new_testlabels, pred_testlabels)\n",
    "    test_acc = accuracy_score(new_testlabels, pred_testlabels>0.5)\n",
    "    print('AUC: %0.04f' % (test_auc))\n",
    "    print('PRAUC: %0.04f' % (test_pr_auc))\n",
    "    print('ACC: %0.04f' % (test_acc))\n",
    "    pre, rec, threshold = precision_recall_curve(new_testlabels, pred_testlabels)\n",
    "    test_pre_rec_f1 = precision_recall_fscore_support(new_testlabels, pred_testlabels > 0.5, average='binary')\n",
    "    print(\"  test Precision, Recall and F1:\\t\\t{:.4f} %\\t\\t{:.4f}\\t\\t{:.4f}\".format(test_pre_rec_f1[0],\n",
    "                                                                                    test_pre_rec_f1[1],\n",
    "                                                                                    test_pre_rec_f1[2]))\n",
    "    epoch = 6\n",
    "    rnn_testlabels = np.load(\"rnn_results/testlabels_\" + str(epoch) + \".npy\")\n",
    "    rnn_pred_testlabels = np.load(\"rnn_results/predlabels_\" + str(epoch) + \".npy\")\n",
    "    pre_rnn, rec_rnn, threshold_rnn = precision_recall_curve(rnn_testlabels, rnn_pred_testlabels)\n",
    "    test_pre_rec_f1 = precision_recall_fscore_support(rnn_testlabels, rnn_pred_testlabels > 0.5, average='binary')\n",
    "    test_auc = roc_auc_score(rnn_testlabels, rnn_pred_testlabels)\n",
    "    test_acc = accuracy_score(rnn_testlabels, rnn_pred_testlabels > 0.5)\n",
    "    print('rnnAUC: %0.04f' % (test_auc))\n",
    "    print('rnnACC: %0.04f' % (test_acc))\n",
    "    print(\"  rnn test Precision, Recall and F1:\\t\\t{:.4f} %\\t\\t{:.4f}\\t\\t{:.4f}\".format(test_pre_rec_f1[0],\n",
    "                                                                                    test_pre_rec_f1[1],\n",
    "                                                                                    test_pre_rec_f1[2]))\n",
    "    epoch = 5\n",
    "    wv_testlabels = np.load(\"rnnwordvec_results/testlabels_\" + str(epoch) + \".npy\")\n",
    "    wv_pred_testlabels = np.load(\"rnnwordvec_results/predlabels_\" + str(epoch) + \".npy\")\n",
    "    pre_wv, rec_wv, threshold_wv = precision_recall_curve(wv_testlabels, wv_pred_testlabels)\n",
    "    test_pre_rec_f1 = precision_recall_fscore_support(new_testlabels, wv_pred_testlabels > 0.5, average='binary')\n",
    "    test_auc = roc_auc_score(wv_testlabels, wv_pred_testlabels)\n",
    "    test_acc = accuracy_score(wv_testlabels, wv_pred_testlabels > 0.5)\n",
    "    print('wvAUC: %0.04f' % (test_auc))\n",
    "    print('wvACC: %0.04f' % (test_acc))\n",
    "    print(\"  wv test Precision, Recall and F1:\\t\\t{:.4f} %\\t\\t{:.4f}\\t\\t{:.4f}\".format(test_pre_rec_f1[0],\n",
    "                                                                                    test_pre_rec_f1[1],\n",
    "                                                                                    test_pre_rec_f1[2]))\n",
    "\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(rec, pre, label='CONTENT')\n",
    "    plt.plot(rec_rnn, pre_rnn, label='RNN')\n",
    "    plt.plot(rec_wv, pre_wv, label='RNN+word2vec')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.title(\"Precision-Recall Curves\")\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def list2dic(_list):\n",
    "    output = dict()\n",
    "    for i in _list:\n",
    "        if i in output:\n",
    "            output[i] +=1\n",
    "        else:\n",
    "            output[i] = 0\n",
    "    return output\n",
    "\n",
    "def outputCodes(indexs, patientList):\n",
    "    HightPat = []\n",
    "    for i in indexs:\n",
    "        HightPat.extend(patientList[i])\n",
    "    high = list2dic(HightPat)\n",
    "    items = sorted(high.items(), key=lambda d: d[1], reverse=True)\n",
    "    for key, value in items[:20]:\n",
    "        print(key,value)\n",
    "\n",
    "\n",
    "def scatter(x, colors):\n",
    "    import matplotlib.patheffects as PathEffects\n",
    "    import seaborn as sns\n",
    "    # We choose a color palette with seaborn.\n",
    "    palette = np.array(sns.color_palette(\"hls\", 50))\n",
    "    # We create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,\n",
    "                    c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    # We add the labels for each digit.\n",
    "    txts = []\n",
    "    # for i in range(5):\n",
    "    #     # Position of each label.\n",
    "    #     xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "    #     txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "    #     txt.set_path_effects([\n",
    "    #         PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "    #         PathEffects.Normal()])\n",
    "    #     txts.append(txt)\n",
    "    return f, ax, sc, txts\n",
    "\n",
    "def clustering(thetaPath, dataset):\n",
    "    from sklearn.cluster import MiniBatchKMeans, SpectralClustering\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    thetas = np.asarray(np.load(thetaPath))[:,50:]\n",
    "    ypred = MiniBatchKMeans(n_clusters=20).fit_predict(thetas).flatten()\n",
    "    tsn = TSNE(random_state=256,n_iter=2000).fit_transform(thetas)\n",
    "    scatter(tsn, ypred)\n",
    "    plt.show()\n",
    "    X_test_data, Y_test_data = data_sets.get_data_from_type(\"test\")\n",
    "    new_X = []\n",
    "    for s in X_test_data:\n",
    "        ss = []\n",
    "        for t in s:\n",
    "            ss.extend(t)\n",
    "        new_X.append(ss)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    for ylabel in range(20):\n",
    "        indexs = np.where(ypred == ylabel)[0]\n",
    "        print(\"Cluster\", ylabel)\n",
    "        outputCodes(indexs, new_X)\n",
    "        n = []\n",
    "        for i in indexs:\n",
    "            n.append(sum(Y_test_data[i]))\n",
    "        n = np.array(n)\n",
    "        aveCount = np.mean(n)\n",
    "        stdev = np.std(n)\n",
    "        print(\"Number of Examples:\\t\",len(indexs))\n",
    "        print(\"Readmission AveCount:\\t\",aveCount)\n",
    "        print(\"Readmission Std:\\t\", stdev)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "    # indexs1 = np.where(ypred==1)[0]\n",
    "    # indexs3 = np.where(ypred==3)[0]\n",
    "    # indexs5 = np.where(ypred == 5)[0]\n",
    "    # indexs9 = np.where(ypred == 9)[0]\n",
    "    # indexs12 = np.where(ypred == 12)[0]\n",
    "    #\n",
    "    # X_test_data, Y_test_data = data_sets.get_data_from_type(\"test\")\n",
    "    # new_X = []\n",
    "    # for s in X_test_data:\n",
    "    #     ss = []\n",
    "    #     for t in s:\n",
    "    #         ss.extend(t)\n",
    "    #     new_X.append(ss)\n",
    "    # outputCodes(indexs1,new_X)\n",
    "    # print(\"\\n\")\n",
    "    #\n",
    "    # outputCodes(indexs3, new_X)\n",
    "    # print(\"\\n\")\n",
    "    # outputCodes(indexs5, new_X)\n",
    "    # print(\"\\n\")\n",
    "    # outputCodes(indexs9, new_X)\n",
    "    # print(\"\\n\")\n",
    "    # outputCodes(indexs12, new_X)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FLAGS = Config()\n",
    "    data_sets = PatientReader(FLAGS)\n",
    "    wordvecPath = os.path.join(FLAGS.data_path, \"word2vec.vector\")\n",
    "    W_embed = loadEmbeddingMatrix(wordvecPath)\n",
    "    # main(data_sets, W_embed)\n",
    "    # eval(2)\n",
    "\n",
    "    thetaPath = \"theta_with_rnnvec/thetas_train0.npy\"\n",
    "    clustering(thetaPath, data_sets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
