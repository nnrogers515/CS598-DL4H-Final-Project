{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE IN THIS FILE IS BASED ON THE FORMAT OF https://github.com/danicaxiao/CONTENT/blob/master/transform.py\n",
    "### HOWEVER THE CODE IS WRITTEN IN MY OWN FORM FOR BETTER UNDERSTANDING\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Definitions\n",
    "\n",
    "# Util Functions From https://github.com/danicaxiao/CONTENT/blob/master/util.py\n",
    "def save_pkl(path, dump):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(dump, file)\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "    \n",
    "def save_npy(path, obj):\n",
    "    np.save(path, obj)\n",
    "\n",
    "def load_npy(path):\n",
    "    obj = np.load(path)\n",
    "    return obj\n",
    "\n",
    "# Embedding Defs\n",
    "RARE_WORD = 100\n",
    "STOP_WORD = 1e4\n",
    "UNKNOWN = 1\n",
    "\n",
    "# General File Paths\n",
    "VOCAB_FILE = \"./data/vocab.txt\"\n",
    "STOP_FILE = \"./data/stop.txt\"\n",
    "VOCAB_PKL = \"./data/vocab.pkl\"\n",
    "SIM_DATA_URL = \"https://journals.plos.org/plosone/article/file?type=supplementary&id=10.1371/journal.pone.0195024.s001\"\n",
    "SIM_DATA_ZIP = \"./txtData.zip\"\n",
    "INPUT_FILE = \"./data/S1_File.txt\"\n",
    "\n",
    "# Train/Validation/Test Data File Paths\n",
    "X_TRAIN_FILE = \"./data/X_train.pkl\"\n",
    "X_VALID_FILE = \"./data/X_valid.pkl\"\n",
    "X_TEST_FILE = \"./data/X_test.pkl\"\n",
    "\n",
    "# Train/Validation/Test Label File Paths\n",
    "Y_TRAIN_FILE = \"./data/Y_train.pkl\"\n",
    "Y_VALID_FILE = \"./data/Y_valid.pkl\"\n",
    "Y_TEST_FILE = \"./data/Y_test.pkl\"\n",
    "\n",
    "# Train/Validation Split Values\n",
    "TRAIN_COUNT = 4000\n",
    "VALID_COUNT = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving Data...\n",
      "\n",
      "    PID  DAY_ID                               DX_GROUP_DESCRIPTION  \\\n",
      "0    1   73888                                    ANGINA PECTORIS   \n",
      "1    1   73888  MONONEURITIS OF UPPER LIMB AND MONONEURITIS MU...   \n",
      "2    1   73888  SYMPTOMS INVOLVING RESPIRATORY SYSTEM AND OTHE...   \n",
      "3    1   73880                                 ACUTE APPENDICITIS   \n",
      "4    1   73880                                  DIABETES MELLITUS   \n",
      "\n",
      "     SERVICE_LOCATION  OP_DATE  \n",
      "0      DOCTORS OFFICE    74084  \n",
      "1      DOCTORS OFFICE    74084  \n",
      "2      DOCTORS OFFICE    74084  \n",
      "3  INPATIENT HOSPITAL    74084  \n",
      "4  INPATIENT HOSPITAL    74084   \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def retrieve_data(print_out=False):\n",
    "    # Retrieve Data If Not In Our Active Directory\n",
    "    if not os.path.exists(SIM_DATA_ZIP):\n",
    "        urllib.request.urlretrieve(SIM_DATA_LOC, SIM_DATA_ZIP)\n",
    "\n",
    "    # Unzip our Data into Usable Form\n",
    "    if not os.path.exists(\"./data\"):\n",
    "        with zipfile.ZipFile(SIM_DATA_ZIP, 'r') as zipped_file:\n",
    "            zipped_file.extractall(\"./data\")\n",
    "\n",
    "\n",
    "    # Read Our Data into a Pandas Table\n",
    "    data = pd.read_csv(INPUT_FILE, sep='\\t', header=0)\n",
    "\n",
    "    # Check Our Data\n",
    "    if print_out:\n",
    "        print(\"\\n\", data.head(), \"\\n\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"Retrieving Data...\")\n",
    "data = retrieve_data(True)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Vocab List to CSV...\n",
      "Done!\n",
      "\n",
      "Writing Stop Word List to CSV...\n",
      "Done!\n",
      "\n",
      "Data Successfully Written as ./data/vocab.txt and ./data/stop.txt in CSV Format!\n"
     ]
    }
   ],
   "source": [
    "def data_to_csv(data):\n",
    "    # Group Our Data By Description\n",
    "    desc = data.groupby('DX_GROUP_DESCRIPTION').size().to_frame('SIZE').reset_index()\n",
    "    rare = desc[desc['SIZE'] > RARE_WORD]\n",
    "    stop = desc[desc['SIZE'] > STOP_WORD]\n",
    "\n",
    "    rare = rare.sort_values(by = 'SIZE').reset_index()['DX_GROUP_DESCRIPTION']\n",
    "    stop = stop.reset_index()['DX_GROUP_DESCRIPTION']\n",
    "        \n",
    "    rare.index += 2 # We will follow the studies format of keeping \"Unknown\" as 1\n",
    "    \n",
    "    print(\"Writing Vocab List to CSV...\")\n",
    "    rare.to_csv(VOCAB_FILE, sep = '\\t', header = False, index = True)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    print(\"\\nWriting Stop Word List to CSV...\")\n",
    "    stop.to_csv(STOP_FILE, sep = '\\t', header = False, index = False)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    print(\"\\nData Successfully Written as {} and {} in CSV Format!\".format(VOCAB_FILE, STOP_FILE))\n",
    "    \n",
    "data_to_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file():\n",
    "    word2ind = {}\n",
    "    \n",
    "    with open(VOCAB_FILE, 'r') as vocab_file:\n",
    "        read_in = csv.reader(vocab_file, delimiter='\\t')\n",
    "        word2ind = { entry[1]:int(entry[0]) for entry in read_in }\n",
    "        \n",
    "    # Save Ind2Word Vec to Pickled File\n",
    "    save_pickle(VOCAB_PKL, {val:key for key, val in word2ind.items()})\n",
    "    \n",
    "    return word2ind\n",
    "\n",
    "# load_data_from_file()\n",
    "# load_pickle(VOCAB_PKL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION IS DIRECTLY RE-USED FROM https://github.com/danicaxiao/CONTENT/blob/master/transform.py\n",
    "def convert_format(word_to_index, events):\n",
    "    # order by PID, DAY_ID\n",
    "    with open(INPUT_FILE, mode='r') as f:\n",
    "        # header\n",
    "        header = f.readline().strip().split('\\t')\n",
    "        print(header)\n",
    "        pos = {}\n",
    "        for key, value in enumerate(header):\n",
    "            pos[value] = key\n",
    "        print(pos)\n",
    "\n",
    "        docs = []\n",
    "        doc = []\n",
    "        sent = []\n",
    "        labels = []\n",
    "        label = []\n",
    "\n",
    "        # init\n",
    "        line = f.readline()\n",
    "        tokens = line.strip().split('\\t')\n",
    "        pid = tokens[pos['PID']]\n",
    "        day_id = tokens[pos['DAY_ID']]\n",
    "        label.append(tag(events, pid, day_id))\n",
    "\n",
    "        while line != '':\n",
    "            tokens = line.strip().split('\\t')\n",
    "            c_pid = tokens[pos['PID']]\n",
    "            c_day_id = tokens[pos['DAY_ID']]\n",
    "\n",
    "            # closure\n",
    "            if c_pid != pid:\n",
    "                doc.append(sent)\n",
    "                docs.append(doc)\n",
    "                sent = []\n",
    "                doc = []\n",
    "                pid = c_pid\n",
    "                day_id = c_day_id\n",
    "                labels.append(label)\n",
    "                label = [tag(events, pid, day_id)]\n",
    "            else:\n",
    "                if c_day_id != day_id:\n",
    "                    doc.append(sent)\n",
    "                    sent = []\n",
    "                    day_id = c_day_id\n",
    "                    label.append(tag(events, pid, day_id))\n",
    "\n",
    "            word = tokens[pos['DX_GROUP_DESCRIPTION']]\n",
    "            try:\n",
    "                sent.append(word_to_index[word])\n",
    "            except KeyError:\n",
    "                sent.append(UNKNOWN)\n",
    "\n",
    "            line = f.readline()\n",
    "\n",
    "        # closure\n",
    "        doc.append(sent)\n",
    "        docs.append(doc)\n",
    "        labels.append(label)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "# THIS FUNCTION IS DIRECTLY RE-USED FROM https://github.com/danicaxiao/CONTENT/blob/master/transform.py\n",
    "def tag(events, pid, day_id):\n",
    "    return 1 if tag_logic(events, pid, day_id) else 0\n",
    "\n",
    "# THIS FUNCTION IS DIRECTLY RE-USED FROM https://github.com/danicaxiao/CONTENT/blob/master/transform.py\n",
    "def tag_logic(events, pid, day_id):\n",
    "    try:\n",
    "        patient = events.loc[int(pid)]\n",
    "\n",
    "        # test whether have events within 30 days\n",
    "        if isinstance(patient, pd.Series):\n",
    "            return (int(day_id) <= patient.DAY_ID) & (patient.DAY_ID < int(day_id) + 30)\n",
    "\n",
    "        return patient.loc[(int(day_id) <= patient.DAY_ID) & (patient.DAY_ID < int(day_id) + 30)].shape[0] > 0\n",
    "    except KeyError:\n",
    "        # the label is not in the [index]\n",
    "        return False\n",
    "\n",
    "# THIS FUNCTION IS DIRECTLY RE-USED FROM https://github.com/danicaxiao/CONTENT/blob/master/transform.py\n",
    "def extract_events():\n",
    "    # extract event \"INPATIENT HOSPITAL\"\n",
    "    target_event = 'INPATIENT HOSPITAL'\n",
    "\n",
    "    df = pd.read_csv(INPUT_FILE, sep='\\t', header=0)\n",
    "    events = df[df['SERVICE_LOCATION'] == target_event]\n",
    "\n",
    "    events = events.groupby(['PID', 'DAY_ID', 'SERVICE_LOCATION']).size().to_frame('COUNT').reset_index()\\\n",
    "        .sort_values(by=['PID', 'DAY_ID'], ascending=True)\\\n",
    "        .set_index('PID')\n",
    "\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(X, labels):\n",
    "    save_pickle(X_TRAIN_FILE, X[:TRAIN_COUNT])\n",
    "    save_pickle(X_VALID_FILE, X[TRAIN_COUNT:(TRAIN_COUNT + VALID_COUNT)])\n",
    "    save_pickle(X_TEST_FILE,  X[TRAIN_COUNT + VALID_COUNT:])\n",
    "    save_pickle(Y_TRAIN_FILE, labels[:TRAIN_COUNT])\n",
    "    save_pickle(Y_VALID_FILE, labels[TRAIN_COUNT:(TRAIN_COUNT + VALID_COUNT)])\n",
    "    save_pickle(Y_TEST_FILE,  labels[TRAIN_COUNT + VALID_COUNT:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell Mimics Main() in https://github.com/danicaxiao/CONTENT/blob/master/transform.py\n",
    "# This Should Setup the Data Needed For Training, Validation, and Testing\n",
    "def main():\n",
    "    data = retrieve_data(True)\n",
    "    data_to_csv(data)\n",
    "    word2ind = load_data_from_file()\n",
    "    events = extract_events()\n",
    "    data, labels = convert_format(word2ind, events)\n",
    "    splits(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    PID  DAY_ID                               DX_GROUP_DESCRIPTION  \\\n",
      "0    1   73888                                    ANGINA PECTORIS   \n",
      "1    1   73888  MONONEURITIS OF UPPER LIMB AND MONONEURITIS MU...   \n",
      "2    1   73888  SYMPTOMS INVOLVING RESPIRATORY SYSTEM AND OTHE...   \n",
      "3    1   73880                                 ACUTE APPENDICITIS   \n",
      "4    1   73880                                  DIABETES MELLITUS   \n",
      "\n",
      "     SERVICE_LOCATION  OP_DATE  \n",
      "0      DOCTORS OFFICE    74084  \n",
      "1      DOCTORS OFFICE    74084  \n",
      "2      DOCTORS OFFICE    74084  \n",
      "3  INPATIENT HOSPITAL    74084  \n",
      "4  INPATIENT HOSPITAL    74084   \n",
      "\n",
      "Writing Vocab List to CSV...\n",
      "Done!\n",
      "\n",
      "Writing Stop Word List to CSV...\n",
      "Done!\n",
      "\n",
      "Data Successfully Written as ./data/vocab.txt and ./data/stop.txt in CSV Format!\n",
      "['PID', 'DAY_ID', 'DX_GROUP_DESCRIPTION', 'SERVICE_LOCATION', 'OP_DATE']\n",
      "{'PID': 0, 'DAY_ID': 1, 'DX_GROUP_DESCRIPTION': 2, 'SERVICE_LOCATION': 3, 'OP_DATE': 4}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
